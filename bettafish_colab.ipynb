{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bettafish - AI Catan Player\n",
    "\n",
    "AlphaBeta search, MCTS, and AlphaZero on a fast bitboard engine.\n",
    "\n",
    "**Runtime**: Use GPU (T4) for neural net training, or high-RAM CPU for multi-core search benchmarks.\n",
    "\n",
    "Go to **Runtime > Change runtime type** and select your preferred hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install uv (fast Python package manager)\n",
    "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "import os\n",
    "os.environ[\"PATH\"] = f\"{os.path.expanduser('~')}/.local/bin:{os.environ['PATH']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repo\n",
    "!git clone https://github.com/Samffprice/bettafish.git\n",
    "os.chdir(\"bettafish\")\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all dependencies (including Cython for the fast bitboard engine)\n",
    "# Uses the system Python (Colab's Python 3.11+)\n",
    "!uv pip install --system -e \".[colab]\" -e \"./catanatron[gym]\" 2>&1 | tail -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Cython extension for the fast bitboard engine\n",
    "!python robottler/bitboard/setup_cython.py build_ext --inplace\n",
    "\n",
    "# Verify it built\n",
    "import importlib\n",
    "from robottler.bitboard import _fast\n",
    "print(f\"Cython module loaded: {_fast.__file__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check hardware\nimport torch\nimport multiprocessing\n\nNCPU = multiprocessing.cpu_count()\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")\nprint(f\"CPU cores: {NCPU}\")\nprint(f\"\\nRecommended --workers: {max(1, NCPU - 1)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Benchmark (Gauntlet)\n\nRun the bitboard search player against baseline opponents.\n\n| Flag | Description |\n|------|-------------|\n| `--bb-search` | Use the fast bitboard search player |\n| `--search-depth N` | Search depth (2 = fast, 3 = strong) |\n| `--blend-weight W` | Neural/heuristic blend (1e8 optimal) |\n| `--dice-sample N` | Sample top-N dice rolls (5 = 3x speedup) |\n| `--games N` | Games per opponent |\n| `--workers N` | Parallel processes (use all cores!) |\n| `--baselines` | Run against all baseline opponents |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import multiprocessing; W = max(1, multiprocessing.cpu_count() - 1)\n\n# Quick benchmark: bitboard search depth 2 vs all baselines (50 games each)\n!python -m robottler.benchmark \\\n    --bb-search \\\n    --search-depth 2 \\\n    --blend-weight 1e8 \\\n    --dice-sample 5 \\\n    --baselines \\\n    --games 50 \\\n    --workers {W}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Strong benchmark: depth 3 (slower but ~72% vs AlphaBeta)\n!python -m robottler.benchmark \\\n    --bb-search \\\n    --search-depth 3 \\\n    --blend-weight 1e8 \\\n    --dice-sample 5 \\\n    --baselines \\\n    --games 20 \\\n    --workers {W}"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. AlphaZero Self-Play Training\n",
    "\n",
    "Generate self-play data with MCTS, then train the dual-head network.\n",
    "\n",
    "This benefits from **GPU** for neural net forward passes during MCTS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate self-play games (adjust --games and --sims for speed vs quality)\n!python -m robottler.az_selfplay generate \\\n    --checkpoint robottler/models/az_iter0.pt \\\n    --games 100 \\\n    --sims 200 \\\n    --output-dir datasets/az_selfplay/colab_gen1 \\\n    --workers {W}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on the generated data\n",
    "!python -m robottler.az_selfplay train \\\n",
    "    --checkpoint robottler/models/az_iter0.pt \\\n",
    "    --data-dir datasets/az_selfplay/colab_gen1 \\\n",
    "    --output robottler/models/az_colab_iter1.pt \\\n",
    "    --epochs 20 \\\n",
    "    --batch-size 256 \\\n",
    "    --lr 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate new checkpoint vs old\n",
    "!python -m robottler.az_selfplay evaluate \\\n",
    "    --new-checkpoint robottler/models/az_colab_iter1.pt \\\n",
    "    --old-checkpoint robottler/models/az_iter0.pt \\\n",
    "    --games 100 \\\n",
    "    --sims 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Full AlphaZero Training Loop\n",
    "\n",
    "Automated generate -> train -> evaluate cycle. This is the long-running job\n",
    "you'd want to run with a GPU runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Full loop: 5 iterations of generate/train/evaluate\n!python -m robottler.az_selfplay loop \\\n    --start-checkpoint robottler/models/az_iter0.pt \\\n    --iterations 5 \\\n    --games-per-iter 200 \\\n    --sims 200 \\\n    --output-dir datasets/az_selfplay/colab_loop \\\n    --epochs 20 \\\n    --eval-games 100 \\\n    --workers {W}"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RL Training (MaskablePPO)\n",
    "\n",
    "Train a policy network with reinforcement learning. Benefits from **multi-core** for\n",
    "parallel environment rollouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m robottler.train_rl \\\n",
    "    --opponent alphabeta \\\n",
    "    --total-steps 200000 \\\n",
    "    --n-envs 8 \\\n",
    "    --bc-model robottler/models/value_net_v2.pt \\\n",
    "    --vps 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Results\n",
    "\n",
    "Download trained models back to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all model checkpoints\n",
    "!ls -lh robottler/models/*.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip models for download\n",
    "!zip -j colab_models.zip robottler/models/az_colab_*.pt robottler/models/az_iter*.pt 2>/dev/null || echo \"No new models yet\"\n",
    "\n",
    "from google.colab import files\n",
    "try:\n",
    "    files.download(\"colab_models.zip\")\n",
    "except:\n",
    "    print(\"Download manually from the file browser (left panel)\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}